{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5592b3f1c6bf91d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Bayesian Data Analysis 2019 - Data Analysis Project Report\n",
    "Anonymous & Anonymous\n",
    "\n",
    "## Analysis problem\n",
    "\n",
    "The aim of this project is to analyse variation from month to month in public transportation. The variations from rush hours to late night are obvious to all of us using public transport. The decision makers are not necessarily analysing these changes between different months, even though it could prove valuable for example, in the maintenance, training and testing of equipment or operators. Therefore we want to look at the monthly variation from multiple years of data.\n",
    "\n",
    "## Loaded packages\n",
    "\n",
    "Below are the loaded packages that are used in the project. Remember to make sure that all the packages are installed into the system (for example, pystan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import pystan\n",
    "import datetime\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Hide warnings to ensure anonymity\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of data\n",
    "\n",
    "The data set used is provided by UCI Machine Learning Repository.\n",
    "\n",
    "The data set is called \"Metro Interstate Traffic Volume Data Set\", and it is described by the provider as follows: \"Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN. Hourly weather features and holidays included for impacts on traffic volume.\"\n",
    "\n",
    "Below we load the data from our csv file. This file is accessible at https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume \n",
    "\n",
    "The part of the data we are the most interested in are the date_time and traffic_volume. From date_time we can easily capture the months and hours of data and from traffic volume, we get the number of passengers passing the specific station starting the mentioned hour. We are going to analyse the working days in order to avoid biases in the distributions and to focus on the high volume rush hour business side of operating the transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "df = pd.read_csv(\"Metro_Interstate_Traffic_Volume.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the data in traffic volume. Below, we can see a histogram of passengers each hour. There are clearly a lot of really slow hours. Then, around 3000 and 4500 there are a couple of spikes. These spikes could be rush hours, last trains going through the station or similar. \n",
    "\n",
    "\n",
    "There do not seem to be clear anomalies. The data is well spread out and the scale seems to fit automatically quite well, i.e. there are no singular values of 10000 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['traffic_volume'], bins=150)\n",
    "plt.title(\"Traffic volume\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will edit our data in order to gain better access into the hourly and monthly attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert date_time-field into datetime format & add index\n",
    "df_with_dates = df\n",
    "df_with_dates['date_time'] = pd.to_datetime(df['date_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_with_dates = df_with_dates.set_index('date_time')\n",
    "df_with_dates['weekday'] = df_with_dates.index.dayofweek\n",
    "df_with_dates['month'] = df_with_dates.index.month\n",
    "df_with_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have created an easily accessible dataframe on the traffic volumes of a certain month at 8 o'clock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get traffic at particular time of the day\n",
    "def traffic_at_time(time, dataframe):\n",
    "    return dataframe.at_time('{}:00'.format(time))['traffic_volume']\n",
    "\n",
    "# make all sublists same size\n",
    "def trim(arr):\n",
    "    min_n = min([len(arr[n]) for n in range(len(arr))])\n",
    "    return [arr[n][:min_n] for n in range(len(arr))]\n",
    "\n",
    "# holds traffic during weekdays at 8\n",
    "traffic_by_month = [traffic_at_time(8, df_with_dates[(df_with_dates['month'] == month) & (df_with_dates['weekday'] < 5)]) \n",
    "                    for month in range(1,13)]\n",
    "\n",
    "\n",
    "traffic_by_month = trim(traffic_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the traffic for all months at 8 o'clock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sns.distplot(traffic_by_month[i], hist=False, kde=True, \n",
    "             bins=int(180/5),\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4}) for i in range(12)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can see that the clear bump on the left tail has disappeared which was present when considering the traffic data for all days. This is most likely because weekends and different times of day are excluded, thus the traffic peaks become more homogenous and form a clear distribution. However, we can clearly see that most of the months are skewed to the left. Additionally, for some of the months, there are \"bumps\" on the left tail of the distribution.\n",
    "\n",
    "As we can see, there are still e.g. many holidays and days with bad weather included in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weather_main'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bumps in the tail are most likely caused by e.g. single days being holidays, causing the amount of traffic to reduce heavily for the day.\n",
    "\n",
    "However, the data is still skewed to the left, requiring us to take that into the account in our prior choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of models\n",
    "\n",
    "Like discussed in the analysis problem, there are different results from the distributions from one month to the other. There might even be multiple peaks in the distributions. We chose to use hierarchical and separate models for our data analysis. Separate is a good choice for the monthly data since we do not want to use all the data together to forecast all the months. Then again, there might be some rules that apply to all the months, for example, every month people tend to go to work around the same time and same days. This creates similarities in the data, that the separate model will not take into account. This is where the hierarchical model comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate model\n",
    "\n",
    "In our separate model, we will use singular months to assess the differences between several months and their parameters $\\theta_j$. This means that we will run our model separately for all the 12 months on a certain hour. Due to the skeweness of the monthly data, we have decided to use the skewed normal distribution. This distribution usually represents the parameters as location $\\xi$, scale $\\omega$ and skew $\\alpha$, but we will be using location $\\mu$, scale $\\sigma$ and skew $\\alpha$ since these are more common for normal distribution and fit better in the discussion language.\n",
    "\n",
    "The model will calculate the probabilities of certain parameters that would represent the data the best. Our observations follow a distribution as follows:\n",
    "\n",
    "$$\\theta_j | y_j \\sim Skewnorm(\\mu_j , \\sigma_j, \\alpha)$$\n",
    "\n",
    "and joint posterior\n",
    "\n",
    "$$p(\\theta|y) \\propto p(y|\\theta)p(\\theta)$$\n",
    "\n",
    "### Hierarchical model\n",
    "\n",
    "In our hierarchical model, we will use singular months and the differences between multiple months to assess the differences between several months and their parameters $\\theta_j$. This means that we will run our model together for all the 12 months on a certain hour and assess the effect of a common hyperparameter $\\tau$.\n",
    "\n",
    "The model will calculate the two kinds of probabilities. First, the same as the separate model: how well parameters represent the data, and second, how well a certain hyperparameter reflects all the parameters of the different months. Therefore, the results will reflect the observations on two levels:\n",
    "1. Level where observations are given the parameters $p(y_{ij}|\\theta_j)$\n",
    "2. Level where parameters are given the hyperparameters $p(\\theta_j|\\tau)$\n",
    "\n",
    "Here the distributions and dependencies are as follows:\n",
    "\n",
    "$$ y_j | \\theta_j \\sim Skewnorm(\\mu_j , \\sigma_j, \\alpha)$$\n",
    "$$\\theta_j | \\tau \\sim Skewnorm(\\mu_0, \\sigma_0, \\alpha)$$\n",
    "\n",
    "and joint posterior\n",
    "\n",
    "$$p(\\theta, \\tau|y) \\propto p(y|\\theta, \\tau)p(\\theta,\\tau) \\propto p(y|\\theta)p(\\theta|\\tau)p(\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior choices\n",
    "\n",
    "We are not sure how the months are actually varying from one to the other. When trying out different values for our distributions and following how well they fit the data plotted before, it seems like the optimal posterior distribution is around $Skewnorm(6000,1300,-3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,7000,10000)[1:-1]\n",
    "dist = ss.skewnorm(loc=6000,scale=1300,a=-3)\n",
    "plt.plot(x, dist.pdf(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the normal distribution and the cauchy distribution for our priors. This is due to the nature of our data. During weekdays, the data follows quite a resembling shape with a long lower-end tail and high volume in the higher values. Due to our desired posterior, we will use weakly informative priors that create $\\mu$, $\\sigma$ and $\\alpha$ values around this distribution.\n",
    "\n",
    "### Separate model\n",
    "\n",
    "For the separate model, we have decided to go with\n",
    "\n",
    "$$\\mu \\sim Norm(6000,10000)$$\n",
    "$$\\sigma \\sim Cauchy(1000,10000)$$\n",
    "$$\\alpha \\sim Norm(0,10000)$$\n",
    "\n",
    "The posterior draws from these priors will be focused around the desired posterior but still with the large variance, our priors will be weakly informative.\n",
    "\n",
    "### Hierarchical model\n",
    "\n",
    "For the hierarchical model, we will first simulate the $\\tau$ values that generate $\\mu$ and $\\sigma$ values. When we simulate values for $\\mu$ and $\\sigma$ around the desired distribution values with a great variance, we can possibly tackle part of the error we would create by choosing informative priors. For the $\\tau$ we have gone with priors:\n",
    "\n",
    "$$\\tau \\sim Norm(\\mu_0,\\sigma_0), where$$\n",
    "\n",
    "$$\\mu_0 \\sim Norm(\\mu_{\\mu0},\\mu_{\\sigma0})$$\n",
    "$$\\mu_{\\mu0} \\sim Norm(6000,10000)$$\n",
    "$$\\mu_{\\sigma0} \\sim Cauchy(1000,10000), and$$\n",
    "\n",
    "$$\\sigma_0 \\sim Cauchy(\\sigma_{\\mu0},\\sigma_{\\sigma0}), where$$\n",
    "$$\\sigma_{\\mu0} \\sim Norm(1000,10000)$$\n",
    "$$\\sigma_{\\sigma0} \\sim Cauchy(1000,10000)$$\n",
    "\n",
    "This way we will generate $\\tau$ values by taking different means and deviations from the different distributions. For $\\alpha$ we have used the same distribution as in the separate model:\n",
    "\n",
    "$$\\alpha \\sim Norm(0,10000)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5592b3f1c6bf91d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## The model and training\n",
    "\n",
    "Next off we are going to introduce our model and train it using STAN.\n",
    "\n",
    "This model is a modified version of \"Comparison of k groups with common variance (ANOVA)\" accessed on 06/12/19 from http://avehtari.github.io/BDA_R_demos/demos_rstan/rstan_demo.html#8_comparison_of_k_groups_with_hierarchical_models\n",
    "\n",
    "### Separate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_model = \"\"\"\n",
    "\n",
    "data {\n",
    "    int<lower=0> N; // number of data points\n",
    "    int<lower=0> K; // number of groups\n",
    "    int<lower=1, upper=K> x[N]; // groups indicator\n",
    "    vector[N] y; // data\n",
    "    \n",
    "    int mu_mu;\n",
    "    int mu_sigma;\n",
    "    int sigma_mu;\n",
    "    int sigma_sigma;\n",
    "    int alpha_mu;\n",
    "    int alpha_sigma;\n",
    "    \n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0>[K] mu;\n",
    "    vector<lower=0>[K] sigma;\n",
    "    real alpha;\n",
    "}\n",
    "\n",
    "model {\n",
    "    mu ~ normal(mu_mu, mu_sigma);\n",
    "    sigma ~ cauchy(sigma_mu, sigma_sigma);\n",
    "    alpha ~ normal(alpha_mu, alpha_sigma);\n",
    "    y ~ skew_normal(mu[x], sigma[x], alpha);\n",
    "}\n",
    "generated quantities {\n",
    "    vector[K] ypred;\n",
    "    vector[N] log_lik;\n",
    "    for (k in 1:K)\n",
    "        ypred[k] = skew_normal_rng(mu[k], sigma[k], alpha);\n",
    "    for (n in 1:N)\n",
    "        log_lik[n] = skew_normal_lpdf(y[n] | mu[x[n]], sigma[x[n]], alpha);\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "separate_stan = pystan.StanModel(model_code=separate_model)\n",
    "\n",
    "traffic_data = traffic_by_month\n",
    "\n",
    "K = len(traffic_data)\n",
    "y = [datapoint for month in traffic_data for datapoint in month]\n",
    "x = [i+1 for i in range(K) for j in range(len(traffic_data[0]))]\n",
    "N = len(x)\n",
    "mu_mu = 6000\n",
    "mu_sigma = 10000\n",
    "sigma_mu = 1000\n",
    "sigma_sigma = 10000\n",
    "alpha_mu = 0\n",
    "alpha_sigma = 10000\n",
    "\n",
    "separate_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mu_mu': mu_mu,\n",
    "    'mu_sigma': mu_sigma,\n",
    "    'sigma_mu': sigma_mu,\n",
    "    'sigma_sigma': sigma_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "separate_fit = separate_stan.sampling(data=separate_data)\n",
    "\n",
    "separate_samples = separate_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sns.distplot(separate_samples['ypred'][:,i],hist=False,bins=30,kde_kws={'linewidth': 2}) for i in range(12)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_model = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N; // number of data points\n",
    "    int<lower=0> K; // number of groups\n",
    "    int<lower=1, upper=K> x[N]; // groups indicator\n",
    "    vector[N] y; // data\n",
    "    \n",
    "    // sorry for the ridiculous naming, basically it goes:\n",
    "    // <parameter name><hyperparameter name>_<hyperparameter's parameter>\n",
    "    \n",
    "    int mumu0_mu;\n",
    "    int mumu0_sigma;\n",
    "    int musigma0_mu;\n",
    "    int musigma0_sigma;\n",
    "    \n",
    "    int sigmamu0_mu;\n",
    "    int sigmamu0_sigma;\n",
    "    int sigmasigma0_mu;\n",
    "    int sigmasigma0_sigma;\n",
    "    \n",
    "    int alpha_mu;\n",
    "    int alpha_sigma;\n",
    "}\n",
    "\n",
    "parameters {    \n",
    "    real mumu0;\n",
    "    real<lower=0> musigma0;\n",
    "    vector<lower=0>[K] mu;\n",
    "    real sigmamu0;\n",
    "    real<lower=0> sigmasigma0;\n",
    "    vector<lower=0>[K] sigma;\n",
    "    real alpha;\n",
    "}\n",
    "model {\n",
    "    mumu0 ~ normal(mumu0_mu, mumu0_sigma);\n",
    "    musigma0 ~ cauchy(musigma0_mu, musigma0_sigma);\n",
    "    mu ~ normal(mumu0, musigma0);\n",
    "    \n",
    "    sigmamu0 ~ normal(sigmamu0_mu, sigmamu0_sigma);\n",
    "    sigmasigma0 ~ cauchy(sigmasigma0_mu, sigmasigma0_sigma);\n",
    "    sigma ~ cauchy(sigmamu0, sigmasigma0);\n",
    "    \n",
    "    alpha ~ normal(alpha_mu, alpha_sigma);\n",
    "    \n",
    "    y ~ skew_normal(mu[x], sigma[x], alpha);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    vector[K] ypred;\n",
    "    vector[N] log_lik;\n",
    "    for (k in 1:K)\n",
    "        ypred[k] = skew_normal_rng(mu[k], sigma[k], alpha);\n",
    "    for (n in 1:N)\n",
    "        log_lik[n] = skew_normal_lpdf(y[n] | mu[x[n]], sigma[x[n]], alpha);\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_stan = pystan.StanModel(model_code=hierarchical_model)\n",
    "\n",
    "traffic_data = traffic_by_month\n",
    "\n",
    "K = len(traffic_data)\n",
    "y = [datapoint for month in traffic_data for datapoint in month]\n",
    "x = [i+1 for i in range(K) for j in range(len(traffic_data[0]))]\n",
    "N = len(x)\n",
    "\n",
    "mumu0_mu = 6000\n",
    "mumu0_sigma = 10000\n",
    "musigma0_mu = 1000\n",
    "musigma0_sigma = 10000\n",
    "sigmamu0_mu = 1000\n",
    "sigmamu0_sigma = 10000\n",
    "sigmasigma0_mu = 1000\n",
    "sigmasigma0_sigma = 10000\n",
    "alpha_mu = 0\n",
    "alpha_sigma = 10000\n",
    "\n",
    "hierarchical_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mumu0_mu': mumu0_mu,\n",
    "    'mumu0_sigma': mumu0_sigma,\n",
    "    'musigma0_mu': musigma0_mu,\n",
    "    'musigma0_sigma': musigma0_sigma,\n",
    "    'sigmamu0_mu': sigmamu0_mu,\n",
    "    'sigmamu0_sigma': sigmamu0_sigma,\n",
    "    'sigmasigma0_mu': sigmasigma0_mu,\n",
    "    'sigmasigma0_sigma': sigmasigma0_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "hierarchical_fit = hierarchical_stan.sampling(data=hierarchical_data)\n",
    "\n",
    "hierarchical_samples = hierarchical_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[sns.distplot(hierarchical_samples['ypred'][:,i], hist=False, kde=True, \n",
    "             bins=int(180/5),\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4}) for i in range(12)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using pystan's inbuilt `check_hmc_diagnostics` to ensure that the models have converged. This means, for example, ensuring that for all the $\\hat{R}$ values for the models it holds that $\\hat{R} < 1.05$. The diagnostics check will run all the necessary tests, such as efficient sampling amount, $\\hat{R}$ values, non-divergence, etc. Since all the checks are true, we can conclude that both models have converged succesfully. **Note that this check will also work on the treedepth.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"separate model:\")\n",
    "print(pystan.check_hmc_diagnostics(separate_fit))\n",
    "print(\"hierarchical model:\")\n",
    "print(pystan.check_hmc_diagnostics(hierarchical_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior predictive checking\n",
    "\n",
    "Our posterior predictive checking will consist of comparing the differences of mean values between the models and actual data. Furthermore, we will conduct qualitative assessment. We will plot the results next to the data and compare visually.\n",
    "\n",
    "Below are the means for each month and average of the means are computed for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_mean_diffs = []\n",
    "separate_mean_diffs = []\n",
    "\n",
    "for i in range(12):\n",
    "    traffic_mean = np.mean(traffic_by_month[i])\n",
    "    hierarchical_mean = np.mean(hierarchical_samples['ypred'][:,i])\n",
    "    separate_mean = np.mean(separate_samples['ypred'][:,i])\n",
    "    \n",
    "    hierarchical_mean_diff = abs(hierarchical_mean - traffic_mean)\n",
    "    separate_mean_diff = abs(separate_mean - traffic_mean)\n",
    "    \n",
    "    hierarchical_mean_diffs.append(hierarchical_mean_diff)\n",
    "    separate_mean_diffs.append(separate_mean_diff)\n",
    "    \n",
    "    print(\"month {}/12 traffic means:\".format(i+1))\n",
    "    print(\"traffic data: {}\".format(round(traffic_mean, 2)))\n",
    "    print(\"hierarchical model: {}, diff: {}\"\n",
    "          .format(round(hierarchical_mean, 2), round(hierarchical_mean_diff, 2)))\n",
    "    print(\"separate model: {}, diff: {}\\n\"\n",
    "          .format(round(separate_mean, 2), round(separate_mean_diff, 2)))\n",
    "\n",
    "average_diff_hierarchical = np.mean(hierarchical_mean_diffs)\n",
    "average_diff_separate = np.mean(separate_mean_diffs)\n",
    "\n",
    "print(\"\\nAverage diff for hierarchical model: {}\".format(round(average_diff_hierarchical, 2)))\n",
    "print(\"\\nAverage diff for separate model: {}\".format(round(average_diff_separate, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this calculation, the difference between the means for each month is small for both models. For the hierarchical model, the mean is slightly smaller.\n",
    "\n",
    "Next off, we will plot the predictions for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    sns.distplot(hierarchical_samples['ypred'][:,i], hist=False,bins=30, kde_kws={'linewidth': 2},\n",
    "                label='hierarchical model')\n",
    "    sns.distplot(separate_samples['ypred'][:,i], hist=False,bins=30, kde_kws={'linewidth': 2},\n",
    "                label='separate model')\n",
    "    sns.distplot(traffic_by_month[i], hist=False, bins=30,kde_kws={'linewidth': 2},\n",
    "                label='traffic data')\n",
    "    plt.title(\"month {}/12\".format(i+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly both of the models resemble the actual data. Both models are slightly skewed, which matches the real data. However, the graph based on the real data has a bumpy tail, which is not simulated by the models. Furthermore, for some months with big bumps, the predictions compensate these bumps with a larger standard deviation and lower peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "It seems like both of our models include a couple of bad k-diagnostics values and the results resemble each other a lot based on the plots above. The PSIS-LOO values are close to each other, as well as SE. **Based on the PSIS-LOO values we should choose the hierarchical model, since it has a greater value.** Furthermore, the k-hat values are lower on the high k-hat points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_inference_data = az.from_pystan(\n",
    "    posterior=separate_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "s = az.loo(separate_inference_data, pointwise=True, scale=\"log\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_inference_data = az.from_pystan(\n",
    "    posterior=hierarchical_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "h = az.loo(hierarchical_inference_data, pointwise=True, scale=\"log\")\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(range(1212), s.pareto_k.data, label=\"Separate model\")\n",
    "plt.scatter(range(1212), h.pareto_k.data, label=\"Hierarchical model\")\n",
    "plt.axhline(y=0.5)\n",
    "plt.axhline(y=0.7)\n",
    "plt.title(\"k_hat-values for each model\")\n",
    "_ =plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.p_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.p_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis\n",
    "\n",
    "As our sensitivity analysis, we will try two different prior choices for the separate model and see how it effects the posterior distribution.\n",
    "\n",
    "Using more specific prior for the separate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mu = 5700\n",
    "mu_sigma = 10\n",
    "sigma_mu = 800\n",
    "sigma_sigma = 10\n",
    "alpha_mu = -5\n",
    "alpha_sigma = 10\n",
    "\n",
    "separate_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mu_mu': mu_mu,\n",
    "    'mu_sigma': mu_sigma,\n",
    "    'sigma_mu': sigma_mu,\n",
    "    'sigma_sigma': sigma_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "separate_fit = separate_stan.sampling(data=separate_data)\n",
    "\n",
    "separate_samples = separate_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_inference_data = az.from_pystan(\n",
    "    posterior=separate_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "s = az.loo(separate_inference_data, pointwise=True, scale=\"log\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using extremely vague prior for the separate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mu = 0\n",
    "mu_sigma = 1000000\n",
    "sigma_mu = 0\n",
    "sigma_sigma = 1000000\n",
    "alpha_mu = 0\n",
    "alpha_sigma = 1000000\n",
    "\n",
    "separate_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mu_mu': mu_mu,\n",
    "    'mu_sigma': mu_sigma,\n",
    "    'sigma_mu': sigma_mu,\n",
    "    'sigma_sigma': sigma_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "separate_fit = separate_stan.sampling(data=separate_data)\n",
    "\n",
    "separate_samples = separate_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_inference_data = az.from_pystan(\n",
    "    posterior=separate_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "s = az.loo(separate_inference_data, pointwise=True, scale=\"log\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly using a very specific informative prior for the separate model leads to overfitting. However, extremely vague prior actually yielded quite good results, which might mean that the model could have used a less informative/noninformative prior.\n",
    "\n",
    "We have not run the sensitivity analysis separately for our hierarchical model. This is due to the fact that running and testing the priors during the project, we have come to the same conclusion as with the separate model sensitivity analysis.\n",
    "\n",
    "Overall, the result of this analysis is that the prior chosen for the separate model is on the edge of being too specific, and the results are dependant on the prior values. Clearly, the priors should not be more informative, and possibly could be a bit weaker. Still, the best results are with our current priors and we will continue with those at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "After all, the models can predict traffic volumes based on the month. Some variation in the monthly tails cause issues for forecasting. There were a lot of problems when implementing the $\\alpha$ into skewed normal distribution and we were not able to get the monthly $\\alpha$ vector to work. In case we were able to fix the $\\alpha$ vector into the model, the tails of the predictions could improve.\n",
    "\n",
    "Our current models are really close to each other and it creates the question whether the models should be more robust and include more data to create more diverse results. A point of improvement would be to create more diverse models. Currently, the predictions our models create seem really similar and the results are close to each other.\n",
    "\n",
    "Our prior choices for the models are mostly logical for the separate model, but for the hierarchical model, the priors could be explored more. Given more time, this could be done and the prior choices could be improved. Furthermore, there are k-hat values over 0.7 every once in a while and this would represent some overfitting. This would suggest to use weaker priors. Our sensitivity analysis supports this conclusion, by ending up with bad results using a more informative prior, and then better results using weaker priors. \n",
    "\n",
    "In the future, one of the things we would work on further, is to include the weather and holidays data into the model. These could possibly explain some of the skewness in, for example, summer times and Christmas when there are holidays. Furthermore, when it is rainy, people will most likely use more public transport instead of walking or cycling to work.\n",
    "\n",
    "Another aspect to think in the future is that currently we are separately predicting 12 different months. For the efficiency of the computation, it could be beneficial to predict only a few problematic months, or months where there would be a lot of maintenance work to be done and the efficiency is the most important aspect. Depending on what the operators want, the public transportation volumes could be used just for those specific needs, whatever the month they want to predict.\n",
    "\n",
    "## Sources\n",
    "\n",
    "Gelman, Carlin, Bunson, Vehtari & Rubin, (2014), \"Bayesian Data Analysis\", Third Edition, Chapman & Hall/CRC\n",
    "\n",
    "Vehtari, Aki, \"BDA_course_Aalto\", accessed on 08/12/2019, accessible from: https://github.com/avehtari/BDA_course_Aalto"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
