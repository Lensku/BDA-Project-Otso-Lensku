{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5592b3f1c6bf91d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Bayesian Data Analysis 2019 - Data Analysis Project Report\n",
    "Anonymous & anonymous\n",
    "\n",
    "## Analysis problem\n",
    "\n",
    "The aim of this project is to analyse variation from month to month in public transportation. The variations from rush hours to late night are obvious to all of us using public transport. The decision makers are not necessarily analysing these changes between different months, even though it could prove valuable for example, in the maintenance, training and testing of equipment or operators. Therefore we want to look at the monthly variation from multiple years of data.\n",
    "\n",
    "## Loaded packages\n",
    "\n",
    "Below are the loaded packages that are used in the project. Remember to make sure that all the packages are installed into the system (for example, pystan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bc55cd248f22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing the libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m from pandas.core.api import (\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;31m# dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_eng_float_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m from pandas.core.index import (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.core.groupby.generic import (  # noqa: F401\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mDataFrameGroupBy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mSeriesGroupBy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_shared_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdispatch_fill_zeros\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconsole\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Series\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0missues\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m26747.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \"\"\"\n\u001b[1;32m---> 59\u001b[1;33m from pandas.plotting._core import (\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0mPlotAccessor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mboxplot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# we can lazily import matplotlib.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matplotlib\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_option\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from pandas.plotting._matplotlib.boxplot import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mBoxPlot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mboxplot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\boxplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint_thing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinePlot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMPLPlot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_get_standard_colors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_flatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_subplots\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_mpl_ge_3_0_0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_get_standard_colors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m from pandas.plotting._matplotlib.tools import (\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0m_flatten\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0m_get_all_lines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\table.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArtist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_rasterization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArtist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfont_manager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFontProperties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLine2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFancyArrowPatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFancyBboxPatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\font_manager.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mafm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mft2font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m from matplotlib.fontconfig_pattern import (\n\u001b[0;32m     42\u001b[0m     parse_fontconfig_pattern, generate_fontconfig_pattern)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\afm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_mathtext_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0muni2type1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import pystan\n",
    "import datetime\n",
    "import arviz as az\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of data\n",
    "\n",
    "The data set used is provided by UCI Machine Learning Repository.\n",
    "\n",
    "The data set is called \"Metro Interstate Traffic Volume Data Set\", and it is described by the provider as follows: \"Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN. Hourly weather features and holidays included for impacts on traffic volume.\"\n",
    "\n",
    "Below we load the data from our csv file. This file is accessible at https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume \n",
    "\n",
    "The part of the data we are the most interested in are the date_time and traffic_volume. From date_time we can easily capture the months and hours of data and from traffic volume, we get the number of passengers passing the specific station starting the mentioned hour. We are going to analyse the working days in order to avoid biases in the distributions and to focus on the business side of operating the transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "df = pd.read_csv(\"Metro_Interstate_Traffic_Volume.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the data in traffic volume. Below, we can see a histogram of passengers each hour. There are clearly a lot of really slow hours. Then, around 3000 and 4500 there are a couple of spikes. These spikes could be rush hours, last trains going through the station or similar, but we are not sure at this point. There do not seem to be clear anomalies, since the data is well spread out and the scale seems to fit automatically quite well, i.e. there are no singular values of 10000 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['traffic_volume'], bins=150)\n",
    "plt.title(\"Traffic volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will edit our data in order to gain better access into the hourly and monthly attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date_time-field into datetime format & add index\n",
    "df_with_dates = df\n",
    "df_with_dates['date_time'] = pd.to_datetime(df['date_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_with_dates = df_with_dates.set_index('date_time')\n",
    "df_with_dates['weekday'] = df_with_dates.index.dayofweek\n",
    "df_with_dates['month'] = df_with_dates.index.month\n",
    "df_with_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traffic_at_time(time, dataframe):\n",
    "    return dataframe.at_time('{}:00'.format(time))['traffic_volume']\n",
    "\n",
    "# inefficient, but makes testing easier\n",
    "traffic_by_time = [traffic_at_time(time, df_with_dates) for time in range(24)]\n",
    "\n",
    "# make all sublists same size\n",
    "def trim(arr):\n",
    "    min_n = min([len(arr[n]) for n in range(len(arr))])\n",
    "    return [arr[n][:min_n] for n in range(len(arr))]\n",
    "\n",
    "traffic_by_time = trim(traffic_by_time)\n",
    "\n",
    "# e.g. traffic volume at 12\n",
    "\n",
    "plt.hist(traffic_by_time[12], bins=50)\n",
    "plt.title(\"Traffic at 12:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have created an easily accessible dataframe on the traffic volumes of a certain month. The histogram below is at 8 o'clock in the weekday mornings in January. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# holds traffic during weekdays at 8\n",
    "traffic_by_month = [traffic_at_time(8, df_with_dates[(df_with_dates['month'] == month) & (df_with_dates['weekday'] < 5)]) \n",
    "                    for month in range(1,13)]\n",
    "\n",
    "traffic_by_month = trim(traffic_by_month)\n",
    "# traffic in january during weekdays at 8\n",
    "\n",
    "[sns.distplot(traffic_by_month[i], hist=False, kde=True, \n",
    "             bins=int(180/5),\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4}) for i in range(12)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis on data\n",
    "\n",
    "In this part we have further analysed the extra data in the dataset. We also discuss some outliers and missing data. These **insights might help us form our distributions later on in the project and analyse the results**. Thus, we see it fit to look at the data even though it is not used. Feel free to skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many labelled data columns in our data. Below we can see the holiday, weather main and weather description labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['weather_main'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['weather_description'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the temperature data we can see that there are clear outliers. The x-axel is scaled from 0 to 300 even though there are barely any values below 250. When we dig deeper below the 250 values, we can see there are a couple of really low values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(df['temp'], bins=50)\n",
    "#plt.title(\"Temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_low_temp = df[df['temp'] < 250]\n",
    "#plt.hist(df_low_temp['temp'], bins=50)\n",
    "#plt.title(\"Low temperatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['temp'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at these zero Kelvin temperatures, we can see that the observations seem otherwise correct. The sky is clear, there is no rain or snow, and there are some small amounts of traffic during that time. Thus, we suggest that these datapoints only lack the temperature data and can still be used. We would fix the points by taking the average temperature during similar times (Jan-Feb nights).\n",
    "\n",
    "Next off is the rain. We can see again that there are some outliers. When we take the description of our data, we see there is a datapoint with around 9831 mm of rain. Otherwise, the data seems reasonable. There are some 50mm rain points, but this is still realistic within one hour of really heavy rain. We would set the 9831mm datapoint to an average of our very heavy rain weather descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(df['rain_1h'], bins=50)\n",
    "#plt.title(\"Rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['weather_description'] == 'very heavy rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rain = df[df['rain_1h'] < 9831]\n",
    "#plt.hist(df_rain['rain_1h'], bins=50)\n",
    "#plt.title(\"Corrected rain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With snow and clouds, the data seems to be correct. There are a lot of datapoints with 0 snow, but there are also a lot of points where there are similar looking numbers of snow. With clouds, the data is quite dispersed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(df['snow_1h'], bins=150)\n",
    "#plt.title(\"Snow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(df['clouds_all'], bins=50)\n",
    "#plt.title(\"Clouds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of models\n",
    "\n",
    "Like discussed in the analysis problem, there are different results from the distributions from one month to the other. There might even be multiple peaks in the distributions. We chose to use hierarchical and separate models for our data analysis. Separate is a good choice for the monthly data since we do not want to use all the data together to forecast all the months. Then again, there might be some rules that apply to all the months, for example, every month people tend to go to work around the same time and same days. This creates similarities in the data, that the separate model will not take into account. This is where the hierarchical model comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate model\n",
    "\n",
    "In our separate model, we will use singular months to assess the differences between several months and their parameters $\\theta_j$. This means that we will run our model separately for all the 12 months on a certain hour. Due to the skeweness of the monthly data, we have decided to use the skewed normal distribution. This distribution usually represents the parameters as location $\\xi$, scale $\\omega$ and skew $\\alpha$, but we will be using location $\\mu$, scale $\\sigma$ and skew $\\alpha$ since these are more common for normal distribution and fit better in the discussion language.\n",
    "\n",
    "The model will calculate the probabilities of certain parameters that would represent the data the best. Our observations follow a distribution as follows:\n",
    "\n",
    "$$\\theta_j | y_j \\sim Skewnorm(\\mu_j , \\sigma_j, \\alpha)$$\n",
    "\n",
    "and joint posterior\n",
    "\n",
    "$$p(\\theta|y) \\propto p(y|\\theta)p(\\theta)$$\n",
    "\n",
    "### Hierarchical model\n",
    "\n",
    "In our hierarchical model, we will use singular months and the differences between multiple months to assess the differences between several months and their parameters $\\theta_j$. This means that we will run our model together for all the 12 months on a certain hour and assess the effect of a common hyperparameter $\\tau$.\n",
    "\n",
    "The model will calculate the two kinds of probabilities. First, the same as the separate model: how well parameters represent the data and second, how well a certain hyperparameter reflects all the parameters of the different months. Therefore, the results will reflect the observations on two levels:\n",
    "1. Level where observations are given the parameters $p(y_{ij}|\\theta_j)$\n",
    "2. Level where parameters are given the hyperparameters $p(\\theta_j|\\tau)$\n",
    "\n",
    "Here the distributions and dependencies are as follows:\n",
    "\n",
    "$$ y_j | \\theta_j \\sim Skewnorm(\\mu_j , \\sigma_j, \\alpha)$$\n",
    "$$\\theta_j | \\tau \\sim Skewnorm(\\mu_0, \\sigma_0, \\alpha)$$\n",
    "\n",
    "and joint posterior\n",
    "\n",
    "$$p(\\theta, \\tau|y) \\propto p(y|\\theta, \\tau)p(\\theta,\\tau) \\propto p(y|\\theta)p(\\theta|\\tau)p(\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior choices\n",
    "\n",
    "We will try out two prior distribution for our use: first one being the normal distribution and the second being a skewed normal distribution. This is due to the nature of our data. During weekdays, the data follows quite a resembling shape with a long lower-end tail and high volume in the higher values.\n",
    "\n",
    "The priors are weakly informative. We are not sure how the months are actually varying from one to the other. When trying out different values for our distributions and following how well they fit the data plotted before, it seems like the optimal posterior distribution is around $Skewnorm(6000,1300,-3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,7000,10000)[1:-1]\n",
    "dist = ss.skewnorm(loc=6000,scale=1300,a=-3)\n",
    "plt.plot(x, dist.pdf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our desired posterior, we will use weakly informative priors that create $\\mu$, $\\sigma$ and $\\alpha$ values around this distribution.\n",
    "\n",
    "### Separate model\n",
    "\n",
    "For the separate model, we have decided to go with\n",
    "\n",
    "$$\\mu \\sim Norm(6000,10000)$$\n",
    "$$\\sigma \\sim Cauchy(1000,10000)$$\n",
    "$$\\alpha \\sim Norm(0,10000)$$\n",
    "\n",
    "The posterior draws from these priors will be focused around the desired posterior but still with the large variance, our priors will be weakly informative.\n",
    "\n",
    "### Hierarchical model\n",
    "\n",
    "For the hierarchical model, we will first simulate the $\\tau$ values that generate $\\mu$ and $\\sigma$ values. When we simulate values for $\\mu$ and $\\sigma$ around the desired distribution values with a great variance, we can possibly tackle part of the error we would create by choosing informative priors. For the $\\tau$ we have gone with priors:\n",
    "\n",
    "$$\\tau \\sim Norm(\\mu_0,\\sigma_0), where$$\n",
    "\n",
    "$$\\mu_0 \\sim Norm(\\mu_{\\mu0},\\mu_{\\sigma0})$$\n",
    "$$\\mu_{\\mu0} \\sim Norm(6000,10000)$$\n",
    "$$\\mu_{\\sigma0} \\sim Cauchy(1000,10000), and$$\n",
    "\n",
    "$$\\sigma_0 \\sim Cauchy(\\sigma_{\\mu0},\\sigma_{\\sigma0}), where$$\n",
    "$$\\sigma_{\\mu0} \\sim Norm(1000,10000)$$\n",
    "$$\\sigma_{\\sigma0} \\sim Cauchy(1000,10000)$$\n",
    "\n",
    "This way we will generate $\\tau$ values by taking different means and deviations from the different distributions. For $\\alpha$ we have used the same distribution as in the separate model:\n",
    "\n",
    "$$\\alpha \\sim Norm(0,10000)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5592b3f1c6bf91d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## The model and training\n",
    "\n",
    "Next off we are going to introduce our model and train it using STAN.\n",
    "\n",
    "This model is a modified version of \"Comparison of k groups with common variance (ANOVA)\" accessed on 06/12/19 from http://avehtari.github.io/BDA_R_demos/demos_rstan/rstan_demo.html#8_comparison_of_k_groups_with_hierarchical_models\n",
    "\n",
    "### Separate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_model = \"\"\"\n",
    "\n",
    "data {\n",
    "    int<lower=0> N; // number of data points\n",
    "    int<lower=0> K; // number of groups\n",
    "    int<lower=1, upper=K> x[N]; // groups indicator\n",
    "    vector[N] y; // data\n",
    "    int mu_mu;\n",
    "    int mu_sigma;\n",
    "    int sigma_mu;\n",
    "    int sigma_sigma;\n",
    "    int alpha_mu;\n",
    "    int alpha_sigma;\n",
    "    \n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0>[K] mu;\n",
    "    vector<lower=0>[K] sigma;\n",
    "    real alpha;\n",
    "}\n",
    "\n",
    "model {\n",
    "    mu ~ normal(mu_mu, mu_sigma);\n",
    "    sigma ~ cauchy(sigma_mu, sigma_sigma);\n",
    "    alpha ~ normal(alpha_mu, alpha_sigma);\n",
    "    y ~ skew_normal(mu[x], sigma[x], alpha);\n",
    "}\n",
    "generated quantities {\n",
    "    vector[K] ypred;\n",
    "    vector[N] log_lik;\n",
    "    for (k in 1:K)\n",
    "        ypred[k] = skew_normal_rng(mu[k], sigma[k], alpha);\n",
    "    for (n in 1:N)\n",
    "        log_lik[n] = skew_normal_lpdf(y[n] | mu[x[n]], sigma[x[n]], alpha);\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "separate_stan = pystan.StanModel(model_code=separate_model)\n",
    "\n",
    "traffic_data = traffic_by_month\n",
    "\n",
    "K = len(traffic_data)\n",
    "y = [datapoint for month in traffic_data for datapoint in month]\n",
    "x = [i+1 for i in range(K) for j in range(len(traffic_data[0]))]\n",
    "N = len(x)\n",
    "mu_mu = 6000\n",
    "mu_sigma = 10000\n",
    "sigma_mu = 1000\n",
    "sigma_sigma = 10000\n",
    "alpha_mu = 0\n",
    "alpha_sigma = 10000\n",
    "\n",
    "separate_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mu_mu': mu_mu,\n",
    "    'mu_sigma': mu_sigma,\n",
    "    'sigma_mu': sigma_mu,\n",
    "    'sigma_sigma': sigma_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "separate_fit = separate_stan.sampling(data=separate_data)\n",
    "\n",
    "separate_samples = separate_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sns.distplot(separate_samples['ypred'][:,i],hist=False,bins=30,kde_kws={'linewidth': 2}) for i in range(12)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_model = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N; // number of data points\n",
    "    int<lower=0> K; // number of groups\n",
    "    int<lower=1, upper=K> x[N]; // groups indicator\n",
    "    vector[N] y; // data\n",
    "    \n",
    "    int mumu0_mu;\n",
    "    int mumu0_sigma;\n",
    "    int musigma0_mu;\n",
    "    int musigma0_sigma;\n",
    "    \n",
    "    int sigmamu0_mu;\n",
    "    int sigmamu0_sigma;\n",
    "    int sigmasigma0_mu;\n",
    "    int sigmasigma0_sigma;\n",
    "    \n",
    "    int alpha_mu;\n",
    "    int alpha_sigma;\n",
    "}\n",
    "\n",
    "parameters {    \n",
    "    real mumu0;\n",
    "    real<lower=0> musigma0;\n",
    "    vector<lower=0>[K] mu;\n",
    "    real sigmamu0;\n",
    "    real<lower=0> sigmasigma0;\n",
    "    vector<lower=0>[K] sigma;\n",
    "    real alpha;\n",
    "}\n",
    "model {\n",
    "    mumu0 ~ normal(mumu0_mu, mumu0_sigma);\n",
    "    musigma0 ~ cauchy(musigma0_mu, musigma0_sigma);\n",
    "    mu ~ normal(mumu0, musigma0);\n",
    "    \n",
    "    sigmamu0 ~ normal(sigmamu0_mu, sigmamu0_sigma);\n",
    "    sigmasigma0 ~ cauchy(sigmasigma0_mu, sigmasigma0_sigma);\n",
    "    sigma ~ cauchy(sigmamu0, sigmasigma0);\n",
    "    \n",
    "    alpha ~ normal(alpha_mu, alpha_sigma);\n",
    "    \n",
    "    y ~ skew_normal(mu[x], sigma[x], alpha);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    vector[K] ypred;\n",
    "    vector[N] log_lik;\n",
    "    for (k in 1:K)\n",
    "        ypred[k] = skew_normal_rng(mu[k], sigma[k], alpha);\n",
    "    for (n in 1:N)\n",
    "        log_lik[n] = skew_normal_lpdf(y[n] | mu[x[n]], sigma[x[n]], alpha);\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_stan = pystan.StanModel(model_code=hierarchical_model)\n",
    "\n",
    "traffic_data = traffic_by_month\n",
    "\n",
    "K = len(traffic_data)\n",
    "y = [datapoint for month in traffic_data for datapoint in month]\n",
    "x = [i+1 for i in range(K) for j in range(len(traffic_data[0]))]\n",
    "N = len(x)\n",
    "\n",
    "mumu0_mu = 6000\n",
    "mumu0_sigma = 10000\n",
    "musigma0_mu = 1000\n",
    "musigma0_sigma = 10000\n",
    "sigmamu0_mu = 1000\n",
    "sigmamu0_sigma = 10000\n",
    "sigmasigma0_mu = 1000\n",
    "sigmasigma0_sigma = 10000\n",
    "alpha_mu = 0\n",
    "alpha_sigma = 10000\n",
    "\n",
    "hierarchical_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mumu0_mu': mumu0_mu,\n",
    "    'mumu0_sigma': mumu0_sigma,\n",
    "    'musigma0_mu': musigma0_mu,\n",
    "    'musigma0_sigma': musigma0_sigma,\n",
    "    'sigmamu0_mu': sigmamu0_mu,\n",
    "    'sigmamu0_sigma': sigmamu0_sigma,\n",
    "    'sigmasigma0_mu': sigmasigma0_mu,\n",
    "    'sigmasigma0_sigma': sigmasigma0_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "hierarchical_fit = hierarchical_stan.sampling(data=hierarchical_data)\n",
    "\n",
    "hierarchical_samples = hierarchical_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[sns.distplot(hierarchical_samples['ypred'][:,i], hist=False, kde=True, \n",
    "             bins=int(180/5),\n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4}) for i in range(12)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pystan's inbuilt `check_hmc_diagnostics` to ensure that the models have converged. \n",
    "\n",
    "This means ensuring that for all the $\\hat{R}$ values for the models it holds that $\\hat{R} < 1.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"separate model:\")\n",
    "print(pystan.check_hmc_diagnostics(separate_fit))\n",
    "print(\"hierarchical model:\")\n",
    "print(pystan.check_hmc_diagnostics(hierarchical_fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Because `'Rhat': True` for both, we can say that both models have converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior predictive checking\n",
    "\n",
    "Our posterior predictive checking will consist of comparing the differences of mean values between the models and actual data. Furthermore, we will conduct qualitative assessment. We will plot the results next to the data and compare visually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the means for each month and average of the means are computed for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_mean_diffs = []\n",
    "separate_mean_diffs = []\n",
    "\n",
    "for i in range(12):\n",
    "    traffic_mean = np.mean(traffic_by_month[i])\n",
    "    hierarchical_mean = np.mean(hierarchical_samples['ypred'][:,i])\n",
    "    separate_mean = np.mean(separate_samples['ypred'][:,i])\n",
    "    \n",
    "    hierarchical_mean_diff = abs(hierarchical_mean - traffic_mean)\n",
    "    separate_mean_diff = abs(separate_mean - traffic_mean)\n",
    "    \n",
    "    hierarchical_mean_diffs.append(hierarchical_mean_diff)\n",
    "    separate_mean_diffs.append(separate_mean_diff)\n",
    "    \n",
    "    print(\"month {}/12 traffic means:\".format(i+1))\n",
    "    print(\"traffic data: {}\".format(round(traffic_mean, 2)))\n",
    "    print(\"hierarchical model: {}, diff: {}\"\n",
    "          .format(round(hierarchical_mean, 2), round(hierarchical_mean_diff, 2)))\n",
    "    print(\"separate model: {}, diff: {}\\n\"\n",
    "          .format(round(separate_mean, 2), round(separate_mean_diff, 2)))\n",
    "\n",
    "average_diff_hierarchical = np.mean(hierarchical_mean_diffs)\n",
    "average_diff_separate = np.mean(separate_mean_diffs)\n",
    "\n",
    "print(\"\\nAverage diff for hierarchical model: {}\".format(round(average_diff_hierarchical, 2)))\n",
    "print(\"\\nAverage diff for separate model: {}\".format(round(average_diff_separate, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this calculation, the difference between the means for each month is small for both models. For the hierarchical model, the mean is slightly smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the predictions for each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    sns.distplot(hierarchical_samples['ypred'][:,i], hist=False,bins=30, kde_kws={'linewidth': 2},\n",
    "                label='hierarchical model')\n",
    "    sns.distplot(separate_samples['ypred'][:,i], hist=False,bins=30, kde_kws={'linewidth': 2},\n",
    "                label='separate model')\n",
    "    sns.distplot(traffic_by_month[i], hist=False, bins=30,kde_kws={'linewidth': 2},\n",
    "                label='traffic data')\n",
    "    plt.title(\"month {}/12\".format(i+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly both of the models resemble the actual data. Both models are slightly skewed, which matches the real data.\n",
    "\n",
    "However, the graph based on the real data has a bumpy tail, which is not simulated by the models. Furthermore, for some months with big bumps this is compensated by larger standard deviation and lower peak. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "It seems like both of our models include a couple of bad k-diagnostics values and the results resemble each other a lot based on the plots above. The PSIS-LOO values are close to each other, as well as SE. **Based on the PSIS-LOO values we should choose the hierarchical model, since it has a greater value.** Furthermore, the k-hat values are lower on the high k-hat points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_inference_data = az.from_pystan(\n",
    "    posterior=separate_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "s = az.loo(separate_inference_data, pointwise=True, scale=\"log\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_inference_data = az.from_pystan(\n",
    "    posterior=hierarchical_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "h = az.loo(hierarchical_inference_data, pointwise=True, scale=\"log\")\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(range(1212), s.pareto_k.data, label=\"Separate model\")\n",
    "plt.scatter(range(1212), h.pareto_k.data, label=\"Hierarchical model\")\n",
    "plt.axhline(y=0.5)\n",
    "plt.axhline(y=0.7)\n",
    "plt.title(\"k_hat-values for each model\")\n",
    "_ =plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.p_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.p_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis\n",
    "\n",
    "As our sensitivity analysis, we will try two different prior choices for the models and see how it effects the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more specific prior for the separate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mu = 5700\n",
    "mu_sigma = 10\n",
    "sigma_mu = 800\n",
    "sigma_sigma = 10\n",
    "alpha_mu = -5\n",
    "alpha_sigma = 10\n",
    "\n",
    "separate_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mu_mu': mu_mu,\n",
    "    'mu_sigma': mu_sigma,\n",
    "    'sigma_mu': sigma_mu,\n",
    "    'sigma_sigma': sigma_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "separate_fit = separate_stan.sampling(data=separate_data)\n",
    "\n",
    "separate_samples = separate_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_inference_data = az.from_pystan(\n",
    "    posterior=separate_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "s = az.loo(separate_inference_data, pointwise=True, scale=\"log\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using extremely vague prior for the separate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mu = 0\n",
    "mu_sigma = 1000000\n",
    "sigma_mu = 0\n",
    "sigma_sigma = 1000000\n",
    "alpha_mu = 0\n",
    "alpha_sigma = 1000000\n",
    "\n",
    "separate_data = {\n",
    "    'N': N,\n",
    "    'K': K,\n",
    "    'y': y,\n",
    "    'x': x,\n",
    "    'mu_mu': mu_mu,\n",
    "    'mu_sigma': mu_sigma,\n",
    "    'sigma_mu': sigma_mu,\n",
    "    'sigma_sigma': sigma_sigma,\n",
    "    'alpha_mu': alpha_mu,\n",
    "    'alpha_sigma': alpha_sigma\n",
    "}\n",
    "\n",
    "separate_fit = separate_stan.sampling(data=separate_data)\n",
    "\n",
    "separate_samples = separate_fit.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separate_inference_data = az.from_pystan(\n",
    "    posterior=separate_fit,\n",
    "    observed_data=['y'],\n",
    "    log_likelihood='log_lik'\n",
    ")\n",
    "\n",
    "s = az.loo(separate_inference_data, pointwise=True, scale=\"log\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly using a very specific informative prior for the separate model leads to overfitting. However, extremely vague prior actually yielded quite good results, which might mean that the model could have used a less informative/noninformative prior.\n",
    "\n",
    "Overall, the result of this analysis is that the prior chosen for the separate model is on the edge of being too specific, and the results are dependant on the prior values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "After all, the models do not represent too many bad values in the cross-validation and they can predict traffic volumes based on the month. Some variation in the monthly tails cause issues for forecasting. There were a lot of problems when implementing the $\\alpha$ into skewed normal distribution and we were not able to get the monthly $\\alpha$ vector to work.\n",
    "\n",
    "Our current models are really close to each other and it creates the question whether the models should be more robust and include more data to create more diverse results.\n",
    "\n",
    "Our prior choices for the models are mostly logical for the separate model, but for the hierarchical model, the priors could be explored more. Given more time, this could be done and the prior choices could be improved.\n",
    "\n",
    "Currently we are separately predicting 12 different months. For the efficiency of the computation, it could be beneficial to predict \n",
    "\n",
    "- predicting only 1 thing (e.g. 1 month) over 12? (maybe future improvements)?\n",
    "\n",
    "### Future improvements\n",
    "\n",
    "First of the things we would work on further, is to include the weather and holidays data into the model. These could possibly explain some of the skewness in, for example, summer times and Christmas when there are holidays. Furthermore, when it is rainy, people will most likely use more public transport instead of walking or cycling to work.\n",
    "\n",
    "Second point of improvement would be to create more diverse models. Currently, the predictions our models create seem really similar and the results are close to each other.\n",
    "\n",
    "Third, in case we were able to fix the $\\alpha$ vector into the model, the tails of the predictions could improve. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
